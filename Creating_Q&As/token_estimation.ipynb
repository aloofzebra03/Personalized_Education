{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71551b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7201a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot Q&A token usage:\n",
      "  prompt_tokens     = 1230\n",
      "  completion_tokens = 322\n",
      "  total_tokens      = 1552\n",
      "\n",
      "Total calls: 5000\n",
      "Estimated Q&A token usage for 1000 profiles, 5 batches each:\n",
      "Estimated total Q&A tokens: 7,760,000\n",
      "  → Prompt-only:    6,150,000\n",
      "  → Completion-only:1,610,000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from config import PROFILE_CSV_PATH, QUESTION_LIST_PATH\n",
    "from engine.qa_generator import batch_questions, personalize_question\n",
    "from prompts.prompt_builder import build_prompt_and_parser\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "usage_log = []\n",
    "_original = client.models.generate_content\n",
    "\n",
    "def _patched_generate_content(*args, **kwargs):\n",
    "    resp = _original(*args, **kwargs)\n",
    "    u = resp.usage_metadata\n",
    "    usage_log.append({\n",
    "        \"prompt_tokens\":     u.prompt_token_count,\n",
    "        \"completion_tokens\": u.candidates_token_count,\n",
    "        \"total_tokens\":      u.total_token_count,\n",
    "    })\n",
    "    return resp\n",
    "\n",
    "client.models.generate_content = _patched_generate_content\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 3) Load profiles & questions\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "df_profiles = pd.read_csv('data/student_profiles.csv', encoding='utf-8')\n",
    "questions   = [q.strip() for q in open('data/questions.txt', \"r\", encoding=\"utf-8\") if q.strip()]\n",
    "question_batches = batch_questions(questions, batch_size=10)\n",
    "\n",
    "prompt, parser = build_prompt_and_parser()\n",
    "\n",
    "\n",
    "def run_one_batch(profile_row: dict, batch: list[str]) -> dict:\n",
    "    student_id = profile_row.get(\"student_id\", \"unknown_student\")\n",
    "    # 5a) Personalize each question\n",
    "    personalized = [personalize_question(q, student_id) for q in batch]\n",
    "    # 5b) Build the question block string\n",
    "    question_block = \"\\n\".join(f\"{i+1}. {q}\" for i, q in enumerate(personalized))\n",
    "    # 5c) Build prompt text exactly\n",
    "    # Unpack the dict:\n",
    "    values = {\n",
    "        \"profile\":   json.dumps(profile_row, indent=2),\n",
    "        \"questions\": question_block\n",
    "    }\n",
    "    prompt_text = prompt.format(**values)\n",
    "\n",
    "    # 5d) Call Gemini (logged by our patch)\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt_text\n",
    "    )\n",
    "    raw = resp.text\n",
    "    try:\n",
    "        parsed = parser.parse(raw)\n",
    "    except Exception:\n",
    "        parsed = None  # we don't actually need the parsed output here\n",
    "    return raw\n",
    "\n",
    "usage_log.clear()\n",
    "sample = df_profiles.iloc[0].to_dict()\n",
    "run_one_batch(sample, question_batches[0])\n",
    "\n",
    "assert len(usage_log) == 1, \"Expected exactly one generate_content call\"\n",
    "pilot = usage_log[0]\n",
    "n_qa_prompt = pilot[\"prompt_tokens\"]\n",
    "n_qa_out    = pilot[\"completion_tokens\"]\n",
    "n_qa_total  = pilot[\"total_tokens\"]\n",
    "\n",
    "print(\"Pilot Q&A token usage:\")\n",
    "print(f\"  prompt_tokens     = {n_qa_prompt}\")\n",
    "print(f\"  completion_tokens = {n_qa_out}\")\n",
    "print(f\"  total_tokens      = {n_qa_total}\")\n",
    "\n",
    "N_profiles   = 1000            # e.g. 1000\n",
    "batches_each = len(question_batches)       # e.g. 4\n",
    "N_calls      = N_profiles * batches_each   # total batch calls\n",
    "\n",
    "estimated_QA_tokens = N_calls * n_qa_total\n",
    "\n",
    "print(f\"\\nTotal calls: {N_calls}\")\n",
    "print(f\"Estimated Q&A token usage for {N_profiles} profiles, {batches_each} batches each: of 10 questions each\")\n",
    "print(f\"Estimated total Q&A tokens: {estimated_QA_tokens:,}\")\n",
    "print(f\"  → Prompt-only:    {N_calls * n_qa_prompt:,}\")\n",
    "print(f\"  → Completion-only:{N_calls * n_qa_out:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952e2d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
