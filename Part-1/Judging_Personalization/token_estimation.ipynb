{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343d18a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9b0b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pilot judging token usage (per 4-QA batch):\n",
      "  prompt_tokens     = 2080\n",
      "  completion_tokens = 110\n",
      "  total_tokens      = 2190\n",
      "\n",
      "Total scoring calls needed: 10000 (10 calls per student)\n",
      "Estimated total tokens needed for judging: 21,900,000\n",
      "  → Prompt-only budget:     20,800,000\n",
      "  → Completion-only budget: 1,100,000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from processor import load_data, generate_score_for_student\n",
    "from config import client, MODEL_NAME;\n",
    "\n",
    "usage_log = []\n",
    "_original_generate = client.models.generate_content\n",
    "\n",
    "def _patched_generate_content(*args, **kwargs):\n",
    "    resp = _original_generate(*args, **kwargs)\n",
    "    u = resp.usage_metadata\n",
    "    usage_log.append({\n",
    "        \"prompt_tokens\":     u.prompt_token_count,\n",
    "        \"completion_tokens\": u.candidates_token_count,\n",
    "        \"total_tokens\":      u.total_token_count,\n",
    "    })\n",
    "    return resp\n",
    "\n",
    "client.models.generate_content = _patched_generate_content\n",
    "\n",
    "profiles_df, qa_groups = load_data(\n",
    "    profiles_path=\"data/langchain_structured_profiles.csv\",\n",
    "    answers_path=\"data/finetune_personalized_answers.csv\"\n",
    ")\n",
    "\n",
    "sample_profile = profiles_df.iloc[0].to_dict()\n",
    "sample_qas     = qa_groups[0]  # list of 4 {question, rag_answer} dicts\n",
    "\n",
    "usage_log.clear()\n",
    "_ = generate_score_for_student(sample_profile, sample_qas)  # logs one call\n",
    "\n",
    "# Ensure we captured the call\n",
    "if not usage_log:\n",
    "    raise RuntimeError(\"No token usage recorded—check that generate_content was patched\")\n",
    "\n",
    "pilot = usage_log[0]\n",
    "n_score_prompt   = pilot[\"prompt_tokens\"]\n",
    "n_score_out      = pilot[\"completion_tokens\"]\n",
    "n_score_total    = pilot[\"total_tokens\"]\n",
    "\n",
    "print(\"Pilot judging token usage (per 4-QA batch):\")\n",
    "print(f\"  prompt_tokens     = {n_score_prompt}\")\n",
    "print(f\"  completion_tokens = {n_score_out}\")\n",
    "print(f\"  total_tokens      = {n_score_total}\")\n",
    "\n",
    "N_profiles        = 1000\n",
    "QAs_per_student   = 40\n",
    "group_size        = len(sample_qas)          # 4 QAs per API call\n",
    "calls_per_student = QAs_per_student // group_size  # = 10\n",
    "total_calls       = N_profiles * calls_per_student  # = 10 000\n",
    "\n",
    "estimated_total      = total_calls * n_score_total\n",
    "estimated_prompt     = total_calls * n_score_prompt\n",
    "estimated_completion = total_calls * n_score_out\n",
    "\n",
    "print(f\"\\nTotal scoring calls needed: {total_calls} \"\n",
    "      f\"({calls_per_student} calls per student)\")\n",
    "print(f\"Estimated total tokens needed for judging: {estimated_total:,}\")\n",
    "print(f\"  → Prompt-only budget:     {estimated_prompt:,}\")\n",
    "print(f\"  → Completion-only budget: {estimated_completion:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f04f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
