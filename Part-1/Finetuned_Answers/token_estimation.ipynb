{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64fd1083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\micromamba\\envs\\langchain\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\aryan\\Desktop\\Personalized_Education\\Personalized_Education\\Finetuned_Answers\\llm.py:39: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
      "  return HuggingFaceEndpoint(\n",
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! input_key is not default parameter.\n",
      "                    input_key was transferred to model_kwargs.\n",
      "                    Please make sure that input_key is what you intended.\n",
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! output_key is not default parameter.\n",
      "                    output_key was transferred to model_kwargs.\n",
      "                    Please make sure that output_key is what you intended.\n",
      "WARNING:langchain_community.llms.huggingface_endpoint:WARNING! headers is not default parameter.\n",
      "                    headers was transferred to model_kwargs.\n",
      "                    Please make sure that headers is what you intended.\n",
      "c:\\micromamba\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Explain the human digestive sy… → in=223, out=513, total=736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\micromamba\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How does reproduction occur in… → in=223, out=84, total=307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\micromamba\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are acids and bases? Give… → in=228, out=409, total=637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\micromamba\\envs\\langchain\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is the function of the re… → in=227, out=513, total=740\n",
      "\n",
      "=== TOKEN USAGE SUMMARY ===\n",
      "Questions run          : 4\n",
      "Sum input tokens       : 901\n",
      "Sum output tokens      : 1519\n",
      "Sum total tokens       : 2420\n",
      "Average tokens / query : 605.00\n",
      "\n",
      "Extrapolation to 1 000 profiles × 40 questions:\n",
      "  Total calls           : 40000\n",
      "  Estimated tokens      : 24,200,000\n"
     ]
    }
   ],
   "source": [
    "# run_single_profile_with_tokens.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from schema import parser\n",
    "from prompt import get_prompt\n",
    "from llm import get_remote_llm\n",
    "from config import PROFILES_CSV, QUESTIONS_TXT, LLAMA_MODEL\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1) load the tokenizer for exactly your model\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    # count with special tokens so it matches generation call\n",
    "    return len(tokenizer.encode(text, add_special_tokens=True))\n",
    "\n",
    "def format_profile(row: pd.Series) -> str:\n",
    "    return \"\\n\".join(f\"{col}: {val}\" for col, val in row.items())\n",
    "\n",
    "def clean_answer(raw: str, question: str) -> str:\n",
    "    idx = raw.lower().find(question.lower())\n",
    "    if idx != -1:\n",
    "        raw = raw[idx + len(question):]\n",
    "    return raw.lstrip(\" :–—\\n\\t\")\n",
    "\n",
    "def run_one_profile(profile_idx: int = 0):\n",
    "    # — load one profile\n",
    "    df = pd.read_csv(PROFILES_CSV)\n",
    "    row = df.iloc[profile_idx]\n",
    "    student_name = row[\"name\"]\n",
    "    student_id   = profile_idx\n",
    "\n",
    "    # — load questions\n",
    "    with open(QUESTIONS_TXT) as f:\n",
    "        questions = [q.strip() for q in f if q.strip()]\n",
    "\n",
    "    # — build your PromptTemplate and chain\n",
    "    prompt_tpl = get_prompt()\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    llm = get_remote_llm()\n",
    "    chain = prompt_tpl | llm\n",
    "\n",
    "    # accumulators\n",
    "    sum_input_tokens = 0\n",
    "    sum_output_tokens = 0\n",
    "    sum_total_tokens = 0\n",
    "\n",
    "    for q in questions:\n",
    "        # 1) render the exact prompt text\n",
    "        prompt_inputs = {\n",
    "            \"student_name\": student_name,\n",
    "            \"student_id\":   student_id,\n",
    "            \"question\":     q,\n",
    "            \"format_instructions\": format_instructions\n",
    "        }\n",
    "        prompt_text = prompt_tpl.format(**prompt_inputs)\n",
    "\n",
    "        # 2) count input tokens\n",
    "        in_toks = count_tokens(prompt_text)\n",
    "\n",
    "        # 3) call the chain\n",
    "        resp = chain.invoke(prompt_inputs)\n",
    "        raw = resp[0] if isinstance(resp, list) else resp\n",
    "        ans = clean_answer(raw, q)\n",
    "\n",
    "        # 4) count output tokens\n",
    "        out_toks = count_tokens(ans)\n",
    "\n",
    "        total_toks = in_toks + out_toks\n",
    "        sum_input_tokens  += in_toks\n",
    "        sum_output_tokens += out_toks\n",
    "        sum_total_tokens  += total_toks\n",
    "\n",
    "        print(f\"Q: {q[:30]}… → in={in_toks}, out={out_toks}, total={total_toks}\")\n",
    "\n",
    "    n = len(questions)\n",
    "    avg_per_q = sum_total_tokens / n\n",
    "    total_calls = 1_000 * 40\n",
    "    est_tokens  = avg_per_q * total_calls\n",
    "\n",
    "    print(\"\\n=== TOKEN USAGE SUMMARY ===\")\n",
    "    print(f\"Questions run          : {n}\")\n",
    "    print(f\"Sum input tokens       : {sum_input_tokens}\")\n",
    "    print(f\"Sum output tokens      : {sum_output_tokens}\")\n",
    "    print(f\"Sum total tokens       : {sum_total_tokens}\")\n",
    "    print(f\"Average tokens / query : {avg_per_q:.2f}\")\n",
    "    print(f\"\\nExtrapolation to 1 000 profiles × 40 questions:\")\n",
    "    print(f\"  Total calls           : {total_calls}\")\n",
    "    print(f\"  Estimated tokens      : {est_tokens:,.0f}\")\n",
    "\n",
    "run_one_profile(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df3820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
