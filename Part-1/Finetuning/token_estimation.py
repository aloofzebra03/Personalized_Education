# -*- coding: utf-8 -*-
"""token_estimation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SNwo4Wv3RTiR-74clcOaKwjH38IIXdCb
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

# Full script to compute *total* finetuning tokens per epoch for any number of profiles

import numpy as np
from datasets import load_dataset
from unsloth import FastLanguageModel
from unsloth.chat_templates import get_chat_template

# 1) Load model & tokenizer together (same you used in training)
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name     = "unsloth/Llama-3.2-3B-Instruct",
    max_seq_length = 2048,
    dtype          = None,
    load_in_4bit   = False,
)

# 2) Apply the Llama-3 chat template
tokenizer = get_chat_template(tokenizer, chat_template="llama3")

# 3) Load your CSV as a HF Dataset
no_profiles = 50
import pandas as pd
from datasets import Dataset

# 1) Load with pandas
df = pd.read_csv("/content/generated_qas.csv")

# 2) Turn into a HF Dataset
dataset = Dataset.from_pandas(df)

# 4) Reconstruct the "text" field exactly as in your finetune notebook
def formatting_prompts_func(examples):
    qs  = examples["question"]
    ans = examples["answer"]
    convos = [
        [
            {"role": "user",      "content": q},
            {"role": "assistant", "content": a}
        ]
        for q, a in zip(qs, ans)
    ]
    texts = [
        tokenizer.apply_chat_template(
            convo,
            tokenize=False,
            add_generation_prompt=False
        )
        for convo in convos
    ]
    return {"text": texts}

dataset = dataset.map(formatting_prompts_func, batched=True)

# 5) Token-count the *entire* prompt+answer string once per example
lengths = [
    len(tokenizer(ex["text"], add_special_tokens=True).input_ids)
    for ex in dataset
]

num_examples = len(dataset)
tokens_per_epoch_50 = sum(lengths)

# 6) Print 50-profile results
print(f"Examples in dataset       : {num_examples}")
print(f"Tokens per epoch (50 profiles) ≃ {tokens_per_epoch_50:,.0f}\n")

# 7) Scale to 1,000 profiles (linear scale factor = 1000/50 = 20)
scale = 1000 / no_profiles
tokens_per_epoch_1000 = tokens_per_epoch_50 * scale

print(f"Tokens per epoch (1,000 profiles) ≃ {tokens_per_epoch_1000:,.0f}\n")

# 8) (Optional) Compute total tokens for E epochs
E = 10  # replace with your TrainingArguments.num_train_epochs
total_tokens_1000 = tokens_per_epoch_1000 * E
print(f"Total finetuning tokens for {E} epochs (1,000 profiles) ≃ {total_tokens_1000:,.0f}")

